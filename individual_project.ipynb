{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n",
    "<h3 style=\"text-align: center;\"><b>Индивидуальный проект. Reinforcement Learning. Deep Q Network</b></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Небольшое введение</b>\n",
    "\n",
    "Определение из Википедии\n",
    "\n",
    "<i>Обучение с подкреплением (англ. reinforcement learning) — один из способов машинного обучения, в ходе которого испытуемая система (агент) обучается, взаимодействуя с некоторой средой. С точки зрения кибернетики, является одним из видов кибернетического эксперимента. Откликом среды (а не специальной системы управления подкреплением, как это происходит в обучении с учителем) на принятые решения являются сигналы подкрепления, поэтому такое обучение является частным случаем обучения с учителем, но учителем является среда или её модель. Также нужно иметь в виду, что некоторые правила подкрепления базируются на неявных учителях, например, в случае искусственной нейронной среды, на одновременной активности формальных нейронов, из-за чего их можно отнести к обучению без учителя.</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://econophysica.ru/upload/medialibrary/2e1/2e1784d383ec807d567db793d20f31f2.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>В качестве цели данного проекта я решил взять обучение сверточной нейросети для игры в \"Atari Breakout\" с использованием алгоритма DQN (Deep Q Network). </b>\n",
    "\n",
    "\n",
    "<center><img src=\"https://openai.com/content/images/2017/06/spaceinvadersbehavior.gif\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но для начала займемся настройкой среды. Библиотека OpenAI Gym предоставляет доступную из коробки среду для игр Atari и многих других, подробнее можно прочитать <a href=\"https://www.gymlibrary.ml/environments/atari/breakout/\">в документации</a>. Однако я в своей работе другую библиотеку <a href=\"https://stable-baselines3.readthedocs.io/en/master/\">Stable Baselines3</a> где помимо среды доступны некоторые обертки над ней, которые используются в <a href=\"https://arxiv.org/pdf/1312.5602.pdf\">статье от DeepMind</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "import torch\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage, VecNormalize\n",
    "import datetime\n",
    "from stable_baselines3.common.logger import configure\n",
    "from typing import Callable\n",
    "\n",
    "# Метод make_atari_env создает среду Breakout с Observation Space 210x160x3, внутри он делает следующие операции:\n",
    "    # NoopReset: obtain initial state by taking random number of no-ops on reset.\n",
    "    # Frame skipping: 4 by default\n",
    "    # Max-pooling: most recent two observations\n",
    "    # Termination signal when a life is lost.\n",
    "    # Resize to a square image: 84x84 by default\n",
    "    # Grayscale observation\n",
    "    # Clip reward to {-1, 0, 1}\n",
    "# Так же можно заметить, что он создает n_envs сред и обучение будет происходить параллельно в несколько потоков\n",
    "envs = make_atari_env(\"ALE/Breakout-v5\", n_envs=4, vec_env_cls=DummyVecEnv)\n",
    "# Тут мы уже можем видить изображения размером 84х84х1\n",
    "print(envs.observation_space.shape)\n",
    "# Трансформируем из формата H, W, C в формат C, H, W\n",
    "envs = VecTransposeImage(envs)\n",
    "print(envs.observation_space.shape)\n",
    "# Собираем несколько наблюдений вместе\n",
    "envs = VecFrameStack(envs, n_stack=4)\n",
    "print(envs.observation_space.shape)\n",
    "# Нормализуем наблюдения, а также награду\n",
    "envs = VecNormalize(envs)\n",
    "print(envs.observation_space.shape)\n",
    "# Таким образом мы имеем доступную из коробки среду, которая полностью удовлетворяет нашему формату и\n",
    "# не требует дальнейших изменений. Т.е. значения Observations и Rewards будут представлены в таком формате,\n",
    "# в котором мы бы хотели их видеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь про сам алгоритм. Наша цель натренировать модель таким образом, чтобы суммарная дисконтированная награда была максимальной. \n",
    "$$R_{t0} = \\sum_{t=t0}^\\infty{\\gamma}^{t-t0}{r_t}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея Q-learning состоит в том, что если мы имеем функцию \n",
    "$$ {Q^*}:{State}\\times{Action} \\rightarrow \\mathbb{R} $$\n",
    "которая может сказать нам, какой будет награда, если мы примем то или иное решение в текущем состоянии, мы можем легко написать функцию политики(Policy function), которая максимизирует нашу награду\n",
    "\n",
    "$$\\pi(s) = \\{\\operatorname{\\argmax}}  Q^*(s,a)$$\n",
    "\n",
    "Однако мы не знаем всего о среде, поэтому у нас нет доступа к $ Q^* $\n",
    "Но, поскольку нейронные сети являются универсальными аппроксиматорами функций, мы можем просто создать их и обучить, чтобы они восстанавливали функцию $ Q^* $.\n",
    "\n",
    "Для нашего обучения мы будем использовать тот факт, что каждая функция $Q$ для некоторой политики подчиняется уравнению Беллмана:\n",
    "$$Q^\\pi (s,a) = r + \\gamma Q^\\pi(s', \\pi(s'))$$\n",
    "\n",
    "Разница правой и левой частью уравнения называется temporal difference error $\\delta$\n",
    "$$\\delta = Q(s,a) - (r+\\gamma\\max_a Q(s',a))$$\n",
    "\n",
    "Чтобы минимизировать эту ошибку, мы будем использовать Huber loss. Huber loss действует как MSE, когда ошибка мала, но как MAE, когда ошибка велика — это делает ее более устойчивой к выбросам, когда оценки $Q$ очень зашумлены. Мы рассчитываем это по батчу \"опытов\" (transitions) $B$, взятых из replay memory:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{|B|} \\sum_{(s,a,s',r) \\in B} \\mathcal{L}(\\delta)$$\n",
    "\n",
    "\n",
    "$$\\mathcal{L}(\\delta) = \\begin{cases}\n",
    "    \\frac{1}{2}(\\delta)^2 & for |\\delta| \\le 1, \\\\\n",
    "    |\\delta| - \\frac{1}{2} & otherwise.\n",
    "    \\end{cases}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем гиперпараметры нашей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENVS = 4\n",
    "GAMMA=0.99\n",
    "BATCH_SIZE=32\n",
    "BUFFER_SIZE=1000000\n",
    "MIN_REPLAY_SIZE=50000\n",
    "EPSILON_END=0.1\n",
    "EPSILON_DECAY=1000000\n",
    "EXPLORATION_FRACTION = 0.5\n",
    "TARGET_UPDATE_FREQ = 10000 // 4\n",
    "LR = 5e-5\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir = \"checkpoints/\" + dt\n",
    "logger = configure(save_dir, [\"stdout\", \"csv\", \"tensorboard\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура используемой нейронной сети:\n",
    "\n",
    "  \n",
    "        (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
    "\n",
    "        (1): ReLU()\n",
    "\n",
    "        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
    "\n",
    "        (3): ReLU()\n",
    "\n",
    "        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
    "\n",
    "        (5): ReLU()\n",
    "        \n",
    "        (6): Flatten(start_dim=1, end_dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим обучение нашей нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(env=envs, learning_rate=LR,buffer_size=BUFFER_SIZE, learning_starts=MIN_REPLAY_SIZE,\\\n",
    "            batch_size=BATCH_SIZE, policy=\"CnnPolicy\", target_update_interval=TARGET_UPDATE_FREQ,\\\n",
    "            device=device, exploration_fraction=EXPLORATION_FRACTION, optimize_memory_usage=True,\\\n",
    "            exploration_final_eps=EPSILON_END, gamma=GAMMA)\n",
    "model.set_logger(logger)\n",
    "\n",
    "model.learn(total_timesteps=1000000, log_interval=4)\n",
    "model.save(save_dir)\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "load_dir = f\"{save_dir}.zip\"\n",
    "# ld = \"checkpoints/2022-07-03T01-32-50.zip\"\n",
    "model = DQN.load(load_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим как играет наша модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env(\"ALE/Breakout-v5\", n_envs=1, vec_env_cls=DummyVecEnv, env_kwargs={\"render_mode\":\"human\"})\n",
    "env = VecTransposeImage(env)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecNormalize(env)\n",
    "model = DQN.load(\"checkpoints/2022-07-03T01-32-50.zip\")\n",
    "obses = env.reset()\n",
    "while True:\n",
    "    actions, _states = model.predict(obses, deterministic=True)\n",
    "    obses, rewards, dones, infos = env.step(actions)\n",
    "    # env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"checkpoints/2022-07-03T01-32-50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Вывод:</b> <i>обученная модель более менее неплохо играет, но т.к. вычислительные мощности не позволяют обучить до уровня серьезного ИИ, модель не показывает сильно впечатляющих результатов. \n",
    "\n",
    "Кода получилось не очень много, т.к. в основном использовались решения, доступные из коробки, но в рамках данного проекта я полностью разобрался как работает этот алгоритм, что находится под капотом у этой библиотеки. Тем не менее, эксперимент был проведен и были получены результаты.  \n",
    "\n",
    "Дальнейшее поле для исследований <a href=\"https://arxiv.org/pdf/1710.02298.pdf\">Rainbow: Combining Improvements in Deep Reinforcement Learning</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mldl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1691e751a3c612ca28907166c08969cdd4ecb15a87dfe3d2704363f5b66775b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
